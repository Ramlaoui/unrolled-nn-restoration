{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    if changed:\n",
    "        raise Exception(\"changed\")\n",
    "except NameError:\n",
    "    changed = False\n",
    "    sys.path.append(\"../\")\n",
    "    os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to show the main parts for how to use the source code for the project Unrolled Optimization Deep Learning for Sparse Signals Restoration with a few results.\n",
    "The experiments that are presented in the report can all be reproduced with the code in this notebook, by changing the configs files used to run the commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Generate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: The datasets are extremely heavy and can take up a lot of hard drive space (around 100Gb for both MassBank and synthetic data). This is due to the sizes of the kernel matrix $H$ which is stored for every signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MassBank Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Ricker Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./data/generate_data.py --Dataset massbank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Ricker kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is used when running the experiments that allow to learn the kernel $H$ with the neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./data/generate_data.py --Dataset massbank_fixed --Fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic data is generated using the notebook `gen_sparse_signal_draft.ipynb`, where the parameters are adapted to the experiments to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic easy dataset\n",
    "\n",
    "This dataset is called easy because there is only one sparsity level that is lower and the noise level is low as well as indicated in the report. However, the kernels are still different inside the dataset and vary in standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gaussian', 'identity']\n",
      "[0.1, 0.2]\n",
      "[0.1, 0.2]\n",
      "8\n",
      "0.1 0.1 gaussian\n",
      "0 th out of 8\n",
      "0.1 0.1 identity\n",
      "10 th out of 8\n",
      "0.1 0.2 gaussian\n",
      "20 th out of 8\n",
      "0.1 0.2 identity\n",
      "30 th out of 8\n",
      "0.2 0.1 gaussian\n",
      "40 th out of 8\n",
      "0.2 0.1 identity\n",
      "50 th out of 8\n",
      "0.2 0.2 gaussian\n",
      "60 th out of 8\n",
      "0.2 0.2 identity\n",
      "70 th out of 8\n"
     ]
    }
   ],
   "source": [
    "!python ./data/generate_synthetic_data.py --dataset_name \"synthetic_easy\" --sparsities 0.01 --num_samples 100 --noise_level 0.5 --kernel_type gaussian identity gaussian_heteroscedastic sparse_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic hard dataset\n",
    "\n",
    "This dataset is called hard because we have a higher sparsity level and a higher noise level, and they are both not fixed. The kernels are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel types:\n",
      "['gaussian', 'identity', 'gaussian_heteroscedastic', 'sparse_random']\n",
      "Noise levels:\n",
      "[0.5, 1.0, 2.0]\n",
      "Sparsities:\n",
      "[0.01, 0.05]\n",
      "24\n",
      "0.01 0.5 gaussian\n",
      "0 th out of 24\n",
      "0.01 0.5 identity\n",
      "20 th out of 24\n",
      "0.01 0.5 gaussian_heteroscedastic\n",
      "40 th out of 24\n",
      "0.01 0.5 sparse_random\n",
      "60 th out of 24\n",
      "0.01 1.0 gaussian\n",
      "80 th out of 24\n",
      "0.01 1.0 identity\n",
      "100 th out of 24\n",
      "0.01 1.0 gaussian_heteroscedastic\n",
      "120 th out of 24\n",
      "0.01 1.0 sparse_random\n",
      "140 th out of 24\n",
      "0.01 2.0 gaussian\n",
      "160 th out of 24\n",
      "0.01 2.0 identity\n",
      "180 th out of 24\n",
      "0.01 2.0 gaussian_heteroscedastic\n",
      "200 th out of 24\n",
      "0.01 2.0 sparse_random\n",
      "220 th out of 24\n",
      "0.05 0.5 gaussian\n",
      "240 th out of 24\n",
      "0.05 0.5 identity\n",
      "260 th out of 24\n",
      "0.05 0.5 gaussian_heteroscedastic\n",
      "280 th out of 24\n",
      "0.05 0.5 sparse_random\n",
      "300 th out of 24\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aliramlaoui/Library/CloudStorage/GoogleDrive-ramlaouiali@gmail.com/My Drive/Cours 3A/AST/project/unrolled-nn-restoration/./data/generate_synthetic_data.py\", line 245, in <module>\n",
      "    np.save(\n",
      "  File \"/Users/aliramlaoui/Library/CloudStorage/GoogleDrive-ramlaouiali@gmail.com/My Drive/Cours 3A/AST/project/unrolled-nn-restoration/venv/lib/python3.10/site-packages/numpy/lib/npyio.py\", line 546, in save\n",
      "    format.write_array(fid, arr, allow_pickle=allow_pickle,\n",
      "  File \"/Users/aliramlaoui/Library/CloudStorage/GoogleDrive-ramlaouiali@gmail.com/My Drive/Cours 3A/AST/project/unrolled-nn-restoration/venv/lib/python3.10/site-packages/numpy/lib/format.py\", line 730, in write_array\n",
      "    array.tofile(fp)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ./data/generate_synthetic_data.py --dataset_name \"synthetic_hard\" --sparsities 0.01 0.05 --num_samples 20 --noise_level 0.5 1 2 --kernel_type gaussian identity gaussian_heteroscedastic sparse_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Kernel type synthetic dataset\n",
    "\n",
    "These datasets are used for running experiments with single kernel type and compare the performances of the models with different kernel types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel types:\n",
      "['identity']\n",
      "Noise levels:\n",
      "[0.2, 0.5, 1.0]\n",
      "Sparsities:\n",
      "[0.01]\n",
      "3\n",
      "0.01 0.2 identity\n",
      "0 th out of 3\n",
      "0.01 0.5 identity\n",
      "10 th out of 3\n",
      "0.01 1.0 identity\n",
      "20 th out of 3\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aliramlaoui/Library/CloudStorage/GoogleDrive-ramlaouiali@gmail.com/My Drive/Cours 3A/AST/project/unrolled-nn-restoration/./data/generate_synthetic_data.py\", line 245, in <module>\n",
      "    np.save(\n",
      "  File \"/Users/aliramlaoui/Library/CloudStorage/GoogleDrive-ramlaouiali@gmail.com/My Drive/Cours 3A/AST/project/unrolled-nn-restoration/venv/lib/python3.10/site-packages/numpy/lib/npyio.py\", line 546, in save\n",
      "    format.write_array(fid, arr, allow_pickle=allow_pickle,\n",
      "  File \"/Users/aliramlaoui/Library/CloudStorage/GoogleDrive-ramlaouiali@gmail.com/My Drive/Cours 3A/AST/project/unrolled-nn-restoration/venv/lib/python3.10/site-packages/numpy/lib/format.py\", line 730, in write_array\n",
      "    array.tofile(fp)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ./data/generate_synthetic_data.py --dataset_name \"synthetic_identity\" --sparsities 0.01 --num_samples 200 --noise_level 0.2 0.5 1 --kernel_type identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./data/generate_synthetic_data.py --dataset_name \"synthetic_gaussian\" --sparsities 0.01 --num_samples 200 --noise_level 0.2 0.5 1 --kernel_type gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./data/generate_synthetic_data.py --dataset_name \"synthetic_gaussian_heteroscedastic\" --sparsities 0.01 --num_samples 200 --noise_level 0.2 0.5 1 --kernel_type gaussian_heteroscedastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./data/generate_synthetic_data.py --dataset_name \"synthetic_sparse_random\" --sparsities 0.01 --num_samples 200 --noise_level 0.2 0.5 1 --kernel_type sparse_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to just train a model with the command line and check the results with Weights and Biases. In the Jupyter notebook, you need to be able to input whether you want to log the training metrics on wandb or not if you can interact with the command line if --is_debug is not set. Otherwise, it is just possible to run directly the line on the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters used as flags here are optional and will just overwrite the parameters in the config file. The config file is the one that is used by default if no config file is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primal Dual model\n",
    "!python -m src.main --config primal_dual --n_epochs 1 --data_path ./data/massbank --batch_size 64 --lr 0.0001 --is_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISTA model\n",
    "!python -m src.main --config ista --n_epochs 1 --data_path ./data/massbank --batch_size 64 --lr 0.0001 --is_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Run an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments are run with the command line and the ones in the report are predefined in the `configs/experiments` folder. After choosing an experiment to run, the command line is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.experiments --config learn_h_default_fixed_ista --is_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will run the experiment with the parameters in the config file, log the results in Weights and Biases if --is_debug is not given and save the model in the folder `models` with the name of the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is then possible to use a trained model and only test it on the test sets that are specified in the corresponding config file. The command line is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.experiments --config learn_h_default_fixed_ista --test --is_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source code is entirely available in this repository and in order to get a more in depth understanding in how the models are implemented, you can check the `src` folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
